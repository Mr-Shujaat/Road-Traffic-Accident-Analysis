{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and Installing Libraries"
      ],
      "metadata": {
        "id": "EBMUCHxBJ21j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "px7WBGCdJHFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ8QQyJ1I1gt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Pycaret and other libraries"
      ],
      "metadata": {
        "id": "EuVCHVsOJQ-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the full version\n",
        "!python -m pip install --upgrade pip setuptools wheel\n",
        "!pip install pycaret\n",
        "\n",
        "!pip install pyyaml==5.4.1\n",
        "\n",
        "!pip install markupsafe==2.0.1\n",
        "\n",
        "!pip install Jinja2\n",
        "\n",
        "!pip install mlflow\n",
        "\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "4RTpzyA1JeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart Runtime"
      ],
      "metadata": {
        "id": "fYY89DgnJyML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "Y3uScFpEKFab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pycaret\n",
        "import jinja2\n",
        "#from pycaret.regression import*\n",
        "from pycaret.classification import*"
      ],
      "metadata": {
        "id": "FwHDUdg4Jzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset"
      ],
      "metadata": {
        "id": "xqvG5bd_KM4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/dataset/KSILatest.csv')\n"
      ],
      "metadata": {
        "id": "sMyvkAmjKP2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = data.sample(frac=0.95, random_state=786).reset_index(drop=True)\n",
        "data_unseen = data.drop(data2.index).reset_index(drop=True)\n",
        "\n",
        "print('Data for Modeling: ' + str(data2.shape))\n",
        "print('Unseen Data For Predictions: ' + str(data_unseen.shape))"
      ],
      "metadata": {
        "id": "E0mGBcEoKa77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment in PyCaret"
      ],
      "metadata": {
        "id": "2Tz8LFjJKi8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.classification import *\n",
        "exp_mclf101 = setup(data, target = 'injury_type', fix_imbalance = True, session_id=123)"
      ],
      "metadata": {
        "id": "m_3pWdCLKk0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "Su4zJpDOKswI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compairing All Models"
      ],
      "metadata": {
        "id": "fnTErdw7K3SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models()"
      ],
      "metadata": {
        "id": "OzoEd82OK6D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Best Model"
      ],
      "metadata": {
        "id": "zOMfLSwoLMcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best= compare_models(n_select = 9, sort= 'Accuracy')"
      ],
      "metadata": {
        "id": "nFwvFivMLRzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Best Models"
      ],
      "metadata": {
        "id": "GxwXOvT5LtUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbc = create_model('gbc')\n",
        "rf = create_model('rf')\n",
        "et = create_model('et')"
      ],
      "metadata": {
        "id": "Vy6mX95_LwGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Models"
      ],
      "metadata": {
        "id": "cxjbbrCDL2io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(gbc)\n",
        "evaluate_model(rf)\n",
        "evaluate_model(et)"
      ],
      "metadata": {
        "id": "vs1xrvD3L5Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Association Rule Mining"
      ],
      "metadata": {
        "id": "SklBAQjsMPLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Apriori"
      ],
      "metadata": {
        "id": "qoenGG9IMUBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install apyori"
      ],
      "metadata": {
        "id": "1VlUma0_Mg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Associations"
      ],
      "metadata": {
        "id": "lIMN9cDPM2jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Assuming your CSV file is named 'accident_data.csv'\n",
        "file_path = '/content/drive/MyDrive/Dataset/Rule 13I.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n",
        "\n",
        "# Convert the DataFrame to a list of lists\n",
        "transactions = df.values.tolist()\n",
        "\n",
        "# Set your desired parameters for the Apriori algorithm\n",
        "min_support = 0.04  # Adjust as needed\n",
        "min_confidence = 0.5  # Adjust as needed\n",
        "\n",
        "# Convert all items in transactions to strings\n",
        "transactions = [[str(item) for item in transaction] for transaction in transactions]\n",
        "\n",
        "# Apply Apriori algorithm\n",
        "rules = apriori(transactions, min_support=min_support, min_confidence=min_confidence)\n",
        "\n",
        "# Convert the rules to a list for easier handling\n",
        "rules_list = list(rules)\n",
        "\n",
        "# Display the discovered association rules\n",
        "for rule in rules_list:\n",
        "    print(rule)\n"
      ],
      "metadata": {
        "id": "oMIhDz4JM4a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting rules into table"
      ],
      "metadata": {
        "id": "kOuy0KfXM5dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "YU3sRumDNHOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Assuming you have already run the Apriori algorithm and obtained the rules_list\n",
        "\n",
        "# Define a function to convert the output to a table format\n",
        "def display_rules(rules_list):\n",
        "    table_headers = [\"Antecedent\", \"Consequent\", \"Support\", \"Confidence\", \"Lift\"]\n",
        "\n",
        "    # Initialize an empty list to store rows of the table\n",
        "    table_rows = []\n",
        "\n",
        "    for rule in rules_list:\n",
        "        antecedent = ', '.join(rule[0]) if rule[0] else 'None'  # Extract antecedent\n",
        "\n",
        "        # Extract consequent information from ordered statistics\n",
        "        consequent_info = rule[2][0]\n",
        "        consequent = ', '.join(consequent_info.items_add) if consequent_info.items_add else 'None'\n",
        "\n",
        "        support = rule[1]\n",
        "        confidence = consequent_info.confidence\n",
        "        lift = consequent_info.lift\n",
        "\n",
        "        # Append the current rule to the table_rows\n",
        "        table_rows.append([antecedent, consequent, support, confidence, lift])\n",
        "\n",
        "    # Display the table using the tabulate library\n",
        "    print(tabulate(table_rows, headers=table_headers, floatfmt=\".4f\"))\n",
        "\n",
        "# Display the rules in a table format\n",
        "display_rules(rules_list)\n"
      ],
      "metadata": {
        "id": "-GX_joCmNIEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Rules into Excel file"
      ],
      "metadata": {
        "id": "U4XwgBudNlGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#excel file\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Assuming you have already run the Apriori algorithm and obtained the rules_list\n",
        "\n",
        "# Define a function to convert the output to a table format\n",
        "def display_rules(rules_list):\n",
        "    table_headers = [\"Antecedent\", \"Consequent\", \"Support\", \"Confidence\", \"Lift\"]\n",
        "\n",
        "    # Initialize an empty list to store rows of the table\n",
        "    table_rows = []\n",
        "\n",
        "    for rule in rules_list:\n",
        "        antecedent = ', '.join(rule[0]) if rule[0] else 'None'  # Extract antecedent\n",
        "\n",
        "        # Extract consequent information from ordered statistics\n",
        "        consequent_info = rule[2][0]\n",
        "        consequent = ', '.join(consequent_info.items_add) if consequent_info.items_add else 'None'\n",
        "\n",
        "        support = rule[1]\n",
        "        confidence = consequent_info.confidence\n",
        "        lift = consequent_info.lift\n",
        "\n",
        "        # Append the current rule to the table_rows\n",
        "        table_rows.append([antecedent, consequent, support, confidence, lift])\n",
        "\n",
        "    # Convert the table data to a DataFrame\n",
        "    df = pd.DataFrame(table_rows, columns=table_headers)\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    excel_file_path = '/content/drive/MyDrive/Dataset/Association Rule 13I.xlsx'\n",
        "    df.to_excel(excel_file_path, index=False)\n",
        "    print(f\"Association rules saved to {excel_file_path}\")\n",
        "\n",
        "# Display the rules in a table format\n",
        "display_rules(rules_list)\n"
      ],
      "metadata": {
        "id": "BkB3aZpONnYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}